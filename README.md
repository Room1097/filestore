# ðŸš€ Filestore: Simple Distributed File System (DFS)

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![Docker](https://img.shields.io/badge/Docker-blue?logo=docker)
![Streamlit](https://img.shields.io/badge/Streamlit-red?logo=streamlit)

A minimal **Distributed File System (DFS)** built in Python â€” designed to demonstrate real-world **distributed systems** concepts like service discovery, replication, fault tolerance, and persistence.

It uses:
- A **metadata server** for coordination  
- Multiple **storage nodes** for distributed file chunks  
- A **Streamlit web client** for easy file upload/download  

All backend components are **containerized with Docker & Docker Compose** for fast deployment and scaling.

---

## ðŸ“˜ Table of Contents

1. [ðŸ’¡ Concepts Demonstrated](#-concepts-demonstrated)  
2. [âœ¨ Key Features](#-key-features)  
3. [ðŸ—ï¸ System Architecture](#%EF%B8%8F-system-architecture)  
   - [Metadata Server](#1-%EF%B8%8F-metadata-server-metadataserverpy)  
   - [Storage Node](#2-%EF%B8%8F-storage-node-storagenodepy)  
   - [Client](#3-%EF%B8%8F-client-clientpy)  
4. [ðŸŒŠ Example Flow](#-example-flow)  
5. [ðŸ How to Run](#-how-to-run)  
6. [ðŸ“š References](#-references)

---

## ðŸ’¡ Concepts Demonstrated

This project provides a hands-on implementation of ideas from **CS401: Distributed and Parallel Computing**, including:

- **Service Discovery:** Storage nodes register dynamically with the metadata server.  
- **Data Chunking:** Files are split into **1 MB chunks** before being stored.  
- **Data Replication:** Each chunk is replicated for durability (`REPLICATION_FACTOR = 2`).  
- **Fault Tolerance:** The metadata server monitors node heartbeats (every 10 s) and removes dead nodes after 30 s.  
- **Centralized Metadata:** A single server manages file-chunk mappings.  
- **Persistence:** Metadata is stored in `metadata.json` to survive restarts.  
- **Clientâ€“Server Architecture:** Separation between the **control plane** (metadata) and **data plane** (storage).

---

## âœ¨ Key Features

- ðŸ“¤ **File Uploading & Downloading** â€” Upload and retrieve files via Streamlit UI  
- ðŸ§© **Chunking** â€” Files split into 1 MB pieces (`CHUNK_SIZE` in `client.py`)  
- ðŸ”„ **Replication** â€” Redundant copies ensure durability  
- â¤ï¸ **Fault Detection** â€” Metadata server tracks heartbeats from nodes  
- ðŸ“’ **Persistent Metadata** â€” State stored in `metadata/metadata.json`  
- ðŸ–¥ï¸ **Web-based UI** â€” Intuitive Streamlit frontend  
- ðŸ³ **Containerized Deployment** â€” Managed through Docker Compose  

---

## ðŸ—ï¸ System Architecture

The DFS consists of three core components:

### 1. ðŸ§  Metadata Server (`metadata_server.py`)

**Role:** The *control plane* (the â€œbrainâ€).  
**Responsibilities:**
- Manages the file namespace  
- Tracks which chunks make up each file  
- Assigns storage nodes for replication  
- Handles node registration and heartbeats  
- Responds to client write and read requests  
- **Default Port:** `6000`

---

### 2. ðŸ—„ï¸ Storage Node (`storage_node.py`)

**Role:** The *data plane* (the â€œworkerâ€).  
**Responsibilities:**
- Registers with the metadata server  
- Sends periodic heartbeats (every 10 s)  
- Stores incoming file chunks on disk  
- Serves `STORE` and `RETRIEVE` requests  
- **Example Ports:** `5001`, `5002`, `5003`

---

### 3. ðŸ’» Client (`client.py`)

**Role:** The *user interface*.  
**Responsibilities:**
- Provides a Streamlit web UI  
- Communicates with the metadata server for file metadata  
- Connects directly to storage nodes for file transfer  

---

## ðŸŒŠ Example Flow

### ðŸ”¼ Uploading a File

1. **Client:** User selects `my_file.txt` in the Streamlit UI.  
2. **Client:** Splits file into chunks (e.g., `chunk-A`, `chunk-B`).  
3. **Client â†’ Metadata Server:** Requests write locations (`GET_WRITE_NODES`).  
4. **Metadata Server:** Responds with replication plan (e.g., nodes 1 & 3).  
5. **Client â†’ Nodes:** Sends `STORE` commands with chunk data.  
6. **Client:** Notifies metadata server once upload completes (`PUT_FILE_INFO`).  

### ðŸ”½ Downloading a File

1. **Client:** User selects `my_file.txt`.  
2. **Client â†’ Metadata Server:** Requests chunk mapping (`GET_FILE_INFO`).  
3. **Metadata Server:** Returns chunk locations.  
4. **Client â†’ Nodes:** Retrieves chunks (`RETRIEVE`).  
5. **Nodes â†’ Client:** Sends chunk data.  
6. **Client:** Reassembles chunks into the original file and serves download.

---

## ðŸ How to Run

### 1. ðŸ§° Prerequisites

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Python 3.9+](https://www.python.org/)
- `pip` (Python package manager)

---

### 2. âš™ï¸ One-Time Setup

#### A. Create `storagenode` Directory
```bash
mkdir storagenode
````

#### B. Create `storagenode/Dockerfile`

```dockerfile
# storagenode/Dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY storage_node.py .
ENTRYPOINT ["python"]
```

#### C. Create `requirements.txt`

```bash
echo "streamlit" > requirements.txt
```

---

### 3. â–¶ï¸ Run the Application

#### Project Structure

```
.
â”œâ”€â”€ client.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile           # Metadata server
â”œâ”€â”€ metadata_server.py
â”œâ”€â”€ requirements.txt     # Contains "streamlit"
â”œâ”€â”€ storage_node.py
â””â”€â”€ storagenode/
    â””â”€â”€ Dockerfile       # Storage node image
```

#### Step 1: Start Backend Services

```bash
docker-compose up -d --build
```

Check running containers:

```bash
docker-compose ps
```

Expected output:

```
metadata-server
storage-node-1
storage-node-2
storage-node-3
```

#### Step 2: Install Client Dependencies

```bash
pip install -r requirements.txt
```

#### Step 3: Run the Streamlit Client

```bash
streamlit run client.py
```

Visit: [http://localhost:8501](http://localhost:8501)

---

### 4. â¹ï¸ Stopping the System

To stop everything safely:

```bash
# Stop Streamlit (Ctrl + C)
docker-compose down
```

This removes the containers and the Docker network
(but preserves persistent data in `metadata/` and `storage/`).

---

## ðŸ“š References

| ID  | Concept                            | Source / Origin                                                     |
| --- | ---------------------------------- | ------------------------------------------------------------------- |
| [1] | Metadata & Fault Tolerance         | Inspired by **GFS (Google File System)** principles                 |
| [2] | Clientâ€“Server Data Flow & Chunking | Adapted from **HDFS (Hadoop Distributed File System)** concepts     |
| [3] | Node Discovery & Heartbeats        | Modeled after standard **Service Discovery** in distributed systems |

---

> ðŸ§  **Educational Purpose:**
> Filestore DFS is designed as a lightweight learning project to explore distributed systems principles like consistency, replication, and fault tolerance â€” similar to how large-scale systems like GFS or HDFS work internally.


